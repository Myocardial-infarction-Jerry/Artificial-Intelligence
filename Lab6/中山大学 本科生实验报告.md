# 中山大学 本科生实验报告

实验课程：人工智能

实验名称：Lab6 神经网络

专业名称：计算机科学与技术

学生姓名：刘森元

学生学号：21307289

实验地点：广州校区东校园 实验中心 D501

报告时间：2023-05-29



***本人使用的环境如下：***

***Apple Inc. MacBook Pro 2021***

***M1 Pro (Apple Silicon)***

***使用的文献、软件、包大部分以超连接形式给出了原址。***



## 一、实验题目

- 在给定文本数据集完成文本情感分类训练，在测试集完成测试，计算准确率。
- 要求
  - 文本的特征可以使用 TF 或 TF-IDF (可以使用sklearn库提取特征)
  - 设计合适的网络结构，选择合适的损失函数利用训练集完成网络训练， 并在测试集上计算准确率
  - 需提交实验报告 + 代码
    - 实验报告应包含损失的可视化展示，以及学习率对准确率影响的可视化展示

## 二、实验内容

本次试验中, 我使用了多层感知机 (DNN) 来完成实验任务.

### 1. 算法原理

#### a) 单层感知机到神经网络

对于单层感知机 (单个神经元), 我们将其定义为一个具有若干输入和输出的模型:

![img](https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220110637351-839081092.png)

该神经元可以从输入与输出之间学习到一个线性关系, 并输出结果
$$
z=\sum_{i=1}^mw_ix_i+b
$$
对于神经元激活函数, 最一般的二元分类有:
$$
\text{sign}(z)=\left\{
\begin{align}
-1,&\quad z<0\\
1,&\quad z\geq 0
\end{align}
\right.
$$
从而得到我们想要的二元结果, 但是该模型仅能用于二元分类, 且无法学习复杂的非线性模型.

而神经网络则在该模型上做出拓展:

- 加入了隐藏层, 隐藏层可以有多层, 能够极大的加强模型的表达能力

  ![img](https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220111519210-2096738104.png)
  
- 增加了输出层的神经元, 这样模型可以拥有多个输出, 灵活地应用于分类回归, 或者其他机器学习领域例如降维与聚类

  ![img](https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220122136538-2002639053.png)

- 对激活函数做出拓展, 加强处理能力, 例如 Sigmoid 函数以及 ReLU 函数
  $$
  \frac1{1+e^{-z}},&\quad Sigmoid\\
  \max(0,z),&\quad ReLU
  $$

#### b) DNN 的基本结构

DNN 结构按照不同层的位置进行划分, 可大致分为三类, 输入层、隐藏层、输出层.

![img](https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220122323148-1704308672.png)

层与层之间是全联接的, 当前层的任意神经元一定与下一层的任意神经元相连, 但是对于局部来说, 单个神经元与感知机一样.

#### c) DNN 的前向传播算法

对于一个输入 $\vec x$, 我们可以看到 DNN 的每一层计算都可以视作为矩阵 $W$.

所以对于当前层的运算, 有
$$
a^1=\vec x\\
a^l=\sigma(z^l)=\sigma(W^la^{l-1}+b^l)
$$

#### d) DNN 的反向传播算法

回归到监督学习的一般问题, 假设我们有样本 $\left\{(x_1,y_1),(x_2,y_2),\cdots \right\}$, 我们需要利用样本训练出模型, 当新的测试样本 $(x',?)$ 到来时, 可以预测 $y'$ 的输出.

采用 DNN 模型, 当我们找到合适的网络结构后, 定义损失函数, 利用梯度下降法, 我们就可以进行迭代优化求最小损失, 以达到模型拟合构建. 此处不再赘述数学推演.

### 2. 伪代码

```pseudocode
Initialize matrix W & vector b
for iter from 1 to n_iter
	for data in datas
		z[0] = data, z[i] = active_function(W[i] * z[i - 1] + b[i])
		loss[L] = loss_function(z[L]), loss[i] = W[i + 1].T * loss[i + 1] · d_active_function(z[i])
	
	W[i] = W[i] - learning_rate * sum(loss[j] * z[j].T)
	b[i] = b[i] - learning_rate * sum(loss[j])
```

### 3. 关键代码展示

```python
class layer(object):

    def __init__(self, in_size, out_size, active_func):
        self.in_size = in_size
        self.out_size = out_size
        self.active_func = active_func
        # Initialize layer as random value
        self.w = np.random.randn(in_size, out_size)
        self.b = np.random.randn(1, out_size)

    def forward(self, x): # DNN forward algorithm
        return self.active_func(np.matmul(x, self.w) + self.b)

      
class DNN(object):

    def __init__(self,
                 layer_shape,
                 active_func=sigmoid,
                 loss_func=mean_square,
                 alpha=0.05,
                 step=100,
                 n_iter=30000,
                 epi=1E-6):
        self.layer_shape = layer_shape
        self.active_func = active_func
        self.loss_func = loss_func
        self.alpha = alpha
        self.step = step
        self.n_iter = n_iter
        self.epi = epi
        self.train_logger = []

        self.layer_num = len(layer_shape) - 1
        self.layers = []
        for index in range(len(layer_shape) - 1):
            self.layers.append(
                layer(layer_shape[index], layer_shape[index + 1], active_func))

    def train(self, datas):
        for it in range(self.n_iter):
            self.train_logger.append(0)
            delta_w = [np.zeros_like(layer.w) for layer in self.layers]
            delta_b = [np.zeros_like(layer.b) for layer in self.layers]
            samples = random.sample(datas, min(len(datas), self.step))
            for data in samples:
                z = loss = [
                    np.zeros([1, self.layers[-1].out_size], dtype=np.float64)
                ] * (self.layer_num + 1)
                x, y = data

                # Forward
                z[-1] = x
                for index in range(self.layer_num):
                    z[index] = self.layers[index].forward(z[index - 1])

                # Backward
                loss[self.layer_num - 1] = self.loss_func(
                    z[self.layer_num - 1], y)
                self.train_logger[-1] += abs(np.sum(loss[self.layer_num - 1]))

                for index in reversed(range(self.layer_num - 1)):
                    loss[index] = np.matmul(
                        loss[index + 1], self.layers[index + 1].w.transpose()
                    ) * self.active_func(z[index], d=1)

                # Delta conuting
                for index in range(self.layer_num):
                    delta_w[index] -= np.matmul(z[index - 1].transpose(),
                                                loss[index])
                    delta_b[index] -= loss[index]

            tag = True
            for index in range(self.layer_num):
                delta_w[index] *= self.alpha
                delta_b[index] *= self.alpha
                tag &= np.max(np.abs(delta_w[index])) < self.epi
            if tag:
                break
            for index in range(self.layer_num):
                self.layers[index].w += delta_w[index]
                self.layers[index].b += delta_b[index]
        showLogger(self.train_logger, self.alpha)

    def predict(self, datas):
        Eval = []
        for data in datas:
            Eval.append(data)
            for layer in self.layers:
                Eval[-1] = layer.forward(Eval[-1])
        return Eval

    ...
```

### 4. 创新点 & 优化

- 将训练集划分为多组, 按组进行训练, 提升了收敛速度
- 利用 `pickle` 可将训练好的模型保存下来, 去除复用时的冗余训练时间
- 实验中使用的各种生成文件均做了标准化处理, 错误均能得到有效反馈

## 三、实验结果及分析

### 1. 实验结果展示示例

![image-20230530110729098](/Users/qiu_nangong/Library/Application Support/typora-user-images/image-20230530110729098.png)

### 2. 评测指标展示及分析

#### Iteration & Loss

对于 Loss 函数, 选用了均方差, 最后的答案 Average Loss 为

![image-20230530104602588](/Users/qiu_nangong/Library/Application Support/typora-user-images/image-20230530104602588.png)

其中准确率的计算方式为: 若当前情感分类向量预期与结果误差值在 0.1 以内, 我们将其认定为分类正确.

#### Learning rate & Accuracy

在此选用 Learning rate 为 {1E-5, 1E-4, 1E-2, 1, 10, 100}

对于不同的 Learning rate, 其 Loss 曲线有

*alpha=1E-5*

![DNN_Loss alpha=0.000010](/Users/qiu_nangong/Documents/GitHub/Artificial-Intelligence/Lab6/res/DNN_Loss alpha=0.000010.png)

*alpha=1E-4*

![DNN_Loss alpha=0.000100](/Users/qiu_nangong/Documents/GitHub/Artificial-Intelligence/Lab6/res/DNN_Loss alpha=0.000100.png)

*alpha=1E-2*

![DNN_Loss alpha=0.010000](/Users/qiu_nangong/Documents/GitHub/Artificial-Intelligence/Lab6/res/DNN_Loss alpha=0.010000.png)

*alpha=1*

![DNN_Loss alpha=1.000000](/Users/qiu_nangong/Documents/GitHub/Artificial-Intelligence/Lab6/res/DNN_Loss alpha=1.000000.png)

*alpha=10*

![DNN_Loss alpha=10.000000](/Users/qiu_nangong/Documents/GitHub/Artificial-Intelligence/Lab6/res/DNN_Loss alpha=10.000000.png)

*alpha=100*

![DNN_Loss alpha=100.000000](/Users/qiu_nangong/Documents/GitHub/Artificial-Intelligence/Lab6/res/DNN_Loss alpha=100.000000.png)

分析可知:

- Learning rate 过小时, 模型不收敛或者收敛速度过慢
- Learning rate 过大时, 模型易出现梯度爆炸或陷入局部最优



***Learning rate & Accuracy***

![](/Users/qiu_nangong/Documents/GitHub/Artificial-Intelligence/Lab6/res/DNN_Loss.png)

## 四、参考资料

[Deep learning - Wikipedia](https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks)

[Gradient descent - Wikipedia](https://en.wikipedia.org/wiki/Gradient_descent)

[Gensim - Github](https://github.com/RaRe-Technologies/gensim)

[深度神经网络 (DNN) 反向传播算法(BP) - cnblogs of Pinard](https://www.cnblogs.com/pinard/p/6422831.html)
